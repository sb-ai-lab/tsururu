{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Initialize TSDataset, Pipeline, Model, Validator, Strategy](#toc1_1_)    \n",
    "    - [TSDataset](#toc1_1_1_)    \n",
    "    - [Pipeline](#toc1_1_2_)    \n",
    "    - [Trainer](#toc1_1_3_)    \n",
    "    - [Strategy](#toc1_1_4_)    \n",
    "- [Save and load checkpoints](#toc2_)    \n",
    "    - [Save checkpoint](#toc2_1_1_)    \n",
    "  - [Load checkpoint for finetune](#toc2_2_)    \n",
    "  - [Load checkpoint for inference](#toc2_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tsururu.dataset import Pipeline, TSDataset\n",
    "from tsururu.model_training import DLTrainer\n",
    "from tsururu.model_training import KFoldCrossValidator\n",
    "from tsururu.models import DLinear_NN\n",
    "from tsururu.strategies import MIMOStrategy\n",
    "from tsururu.transformers import (\n",
    "    DateSeasonsGenerator,\n",
    "    LagTransformer,\n",
    "    SequentialTransformer,\n",
    "    TargetGenerator,\n",
    "    UnionTransformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(\n",
    "    cv: int,\n",
    "    regime: str,\n",
    "    y_true: Optional[List[np.ndarray]] = None,\n",
    "    y_pred: Optional[List[np.ndarray]] = None,\n",
    "    ids: Optional[List[Union[float, str]]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    def _get_fold_value(\n",
    "        value: Optional[Union[float, np.ndarray]], idx: int\n",
    "    ) -> List[Optional[Union[float, np.ndarray]]]:\n",
    "        if value is None:\n",
    "            return [None]\n",
    "        if isinstance(value[idx], float):\n",
    "            return value[idx]\n",
    "        if isinstance(value[idx], np.ndarray):\n",
    "            return value[idx].reshape(-1)\n",
    "        raise TypeError(f\"Unexpected value type. Value: {value}\")\n",
    "\n",
    "    df_res_dict = {}\n",
    "\n",
    "    for idx_fold in range(cv):\n",
    "        # Fill df_res_dict\n",
    "        for name, value in [(\"y_true\", y_true), (\"y_pred\", y_pred)]:\n",
    "            df_res_dict[f\"{name}_{idx_fold+1}\"] = _get_fold_value(value, idx_fold)\n",
    "        if regime != \"local\":\n",
    "            df_res_dict[f\"id_{idx_fold+1}\"] = _get_fold_value(ids, idx_fold)\n",
    "\n",
    "    # Save datasets to specified directory\n",
    "    df_res = pd.DataFrame(df_res_dict)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Initialize TSDataset, Pipeline, Model, Validator, Strategy](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization of the main components is exactly the same as when using ML models. The only difference is that `DLTrainer` allows you to pass many more parameters compared to `MLTrainer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_1_'></a>[TSDataset](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = Path(\"../datasets/global/simulated_data_to_check.csv\")\n",
    "\n",
    "dataset_params = {\n",
    "    \"target\": {\n",
    "        \"columns\": [\"value\"],\n",
    "        \"type\": \"continuous\",\n",
    "    },\n",
    "    \"date\": {\n",
    "        \"columns\": [\"date\"],\n",
    "        \"type\": \"datetime\",\n",
    "    },\n",
    "    \"id\": {\n",
    "        \"columns\": [\"id\"],\n",
    "        \"type\": \"categorical\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq: Day; period: 1\n"
     ]
    }
   ],
   "source": [
    "dataset = TSDataset(\n",
    "    data=pd.read_csv(df_path),\n",
    "    columns_params=dataset_params,\n",
    "    print_freq_period_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_2_'></a>[Pipeline](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = LagTransformer(lags=7)\n",
    "target_generator = TargetGenerator()\n",
    "date_features = DateSeasonsGenerator()\n",
    "date_lags = LagTransformer(lags=7)\n",
    "\n",
    "union_1 = UnionTransformer(transformers_list=[lag, target_generator])\n",
    "seq_1 = SequentialTransformer(transformers_list=[union_1], input_features=[\"value\"])\n",
    "seq_2 = SequentialTransformer(transformers_list=[date_features, date_lags], input_features=[\"date\"])\n",
    "union = UnionTransformer(transformers_list=[seq_1, seq_2])\n",
    "\n",
    "pipeline = Pipeline(union, multivariate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_3_'></a>[Trainer](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the available architectures are `DLinear`, `PatchTST`, `GPT2`, `CycleNet`, `TimesNet`. Moreover, adding your own architecture is quite simple if you follow the logic of the base model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model parameters\n",
    "model = DLinear_NN  # DLinear_NN, PatchTST_NN, GPT4TS_NN, CycleNet_NN, TimesNet_NN\n",
    "model_params = {\"moving_avg\": 7, \"individual\": False}\n",
    "\n",
    "# Configure the validation parameters\n",
    "validation = KFoldCrossValidator\n",
    "validation_params = {\n",
    "    \"n_splits\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheduler with num_steps != num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since users donâ€™t have direct access to len(dataset), Tsururu provides custom relative parameters inside `scheduler_params` to correct schedulers' parameters:\n",
    "\n",
    "- `relative_steps_per_epoch`: defines a multiplier for iterations in one epoch. Used for parameters like steps_per_epoch in OneCycleLR.\n",
    "  - $absolute\\_steps\\_per\\_epoch = relative\\_steps\\_per\\_epoch * (len(dataset) // batch\\_size + 1)$\n",
    "\n",
    "- `relative_steps`: defines a multiplier of the total number of iterations across all epochs. Used for parameters, such as T_max in CosineAnnealingLR.\n",
    "    - $absolute\\_steps = relative\\_steps * n\\_epochs * (len(dataset) // batch\\_size + 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Default Scheduler (no custom parameters needed):\n",
    "scheduler = torch.optim.lr_scheduler.StepLR\n",
    "scheduler_after_epoch = True\n",
    "scheduler_params = {\n",
    "    \"step_size\": 2,\n",
    "}\n",
    "\n",
    "# 2) OneCycleLR using relative_steps_per_epoch:\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR\n",
    "scheduler_after_epoch = False\n",
    "scheduler_params = {\n",
    "    \"relative_steps_per_epoch\": [\"steps_per_epoch\"],\n",
    "    \"steps_per_epoch\": 1.0,\n",
    "    \"epochs\": 10,\n",
    "    \"max_lr\": 0.01,\n",
    "}\n",
    "\n",
    "# 3) \n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "scheduler_after_epoch = False\n",
    "scheduler_params = {\n",
    "    \"relative_steps\": [\"T_max\"],\n",
    "    \"T_max\": 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_params = {\n",
    "    \"device\": \"cpu\",\n",
    "    \"num_workers\": 0,\n",
    "    \"best_by_metric\": True,\n",
    "    \"save_to_dir\": False,\n",
    "    \"n_epochs\": 1,\n",
    "    \"scheduler\": scheduler,\n",
    "    \"scheduler_after_epoch\": scheduler_after_epoch,\n",
    "    \"scheduler_params\": scheduler_params,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "trainer = DLTrainer(\n",
    "    model, \n",
    "    model_params, \n",
    "    validation, \n",
    "    validation_params, \n",
    "    **trainer_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_4_'></a>[Strategy](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 7\n",
    "model_horizon = 7\n",
    "history = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = MIMOStrategy(\n",
    "    pipeline=pipeline,\n",
    "    trainer=trainer,\n",
    "    horizon=horizon,\n",
    "    history=history,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train dataset: 4935\n",
      "length of val dataset: 4935\n",
      "Updating learning rate to 0.001000.\n",
      "Updating learning rate to 0.001000.\n",
      "Updating learning rate to 0.000999.\n",
      "Updating learning rate to 0.000998.\n",
      "Updating learning rate to 0.000997.\n",
      "Updating learning rate to 0.000996.\n",
      "Updating learning rate to 0.000995.\n",
      "Updating learning rate to 0.000993.\n",
      "Updating learning rate to 0.000992.\n",
      "Updating learning rate to 0.000990.\n",
      "Updating learning rate to 0.000988.\n",
      "Updating learning rate to 0.000985.\n",
      "Updating learning rate to 0.000983.\n",
      "Updating learning rate to 0.000980.\n",
      "Updating learning rate to 0.000977.\n",
      "Updating learning rate to 0.000974.\n",
      "Updating learning rate to 0.000971.\n",
      "Updating learning rate to 0.000967.\n",
      "Updating learning rate to 0.000963.\n",
      "Updating learning rate to 0.000959.\n",
      "Updating learning rate to 0.000955.\n",
      "Updating learning rate to 0.000951.\n",
      "Updating learning rate to 0.000947.\n",
      "Updating learning rate to 0.000942.\n",
      "Updating learning rate to 0.000937.\n",
      "Updating learning rate to 0.000932.\n",
      "Updating learning rate to 0.000927.\n",
      "Updating learning rate to 0.000922.\n",
      "Updating learning rate to 0.000916.\n",
      "Updating learning rate to 0.000910.\n",
      "Updating learning rate to 0.000905.\n",
      "Updating learning rate to 0.000898.\n",
      "Updating learning rate to 0.000892.\n",
      "Updating learning rate to 0.000886.\n",
      "Updating learning rate to 0.000879.\n",
      "Updating learning rate to 0.000873.\n",
      "Updating learning rate to 0.000866.\n",
      "Updating learning rate to 0.000859.\n",
      "Updating learning rate to 0.000852.\n",
      "Updating learning rate to 0.000844.\n",
      "Updating learning rate to 0.000837.\n",
      "Updating learning rate to 0.000830.\n",
      "Updating learning rate to 0.000822.\n",
      "Updating learning rate to 0.000814.\n",
      "Updating learning rate to 0.000806.\n",
      "Updating learning rate to 0.000798.\n",
      "Updating learning rate to 0.000790.\n",
      "Updating learning rate to 0.000781.\n",
      "Updating learning rate to 0.000773.\n",
      "Updating learning rate to 0.000764.\n",
      "Updating learning rate to 0.000756.\n",
      "Updating learning rate to 0.000747.\n",
      "Updating learning rate to 0.000738.\n",
      "Updating learning rate to 0.000729.\n",
      "Updating learning rate to 0.000720.\n",
      "Updating learning rate to 0.000711.\n",
      "Updating learning rate to 0.000702.\n",
      "Updating learning rate to 0.000693.\n",
      "Updating learning rate to 0.000683.\n",
      "Updating learning rate to 0.000674.\n",
      "Updating learning rate to 0.000664.\n",
      "Updating learning rate to 0.000655.\n",
      "Updating learning rate to 0.000645.\n",
      "Updating learning rate to 0.000635.\n",
      "Updating learning rate to 0.000625.\n",
      "Updating learning rate to 0.000615.\n",
      "Updating learning rate to 0.000606.\n",
      "Updating learning rate to 0.000596.\n",
      "Updating learning rate to 0.000586.\n",
      "Updating learning rate to 0.000576.\n",
      "Updating learning rate to 0.000566.\n",
      "Updating learning rate to 0.000556.\n",
      "Updating learning rate to 0.000546.\n",
      "Updating learning rate to 0.000535.\n",
      "Updating learning rate to 0.000525.\n",
      "Updating learning rate to 0.000515.\n",
      "Updating learning rate to 0.000505.\n",
      "Updating learning rate to 0.000495.\n",
      "Updating learning rate to 0.000485.\n",
      "Updating learning rate to 0.000475.\n",
      "Updating learning rate to 0.000465.\n",
      "Updating learning rate to 0.000454.\n",
      "Updating learning rate to 0.000444.\n",
      "Updating learning rate to 0.000434.\n",
      "Updating learning rate to 0.000424.\n",
      "Updating learning rate to 0.000414.\n",
      "Updating learning rate to 0.000404.\n",
      "Updating learning rate to 0.000394.\n",
      "Updating learning rate to 0.000385.\n",
      "Updating learning rate to 0.000375.\n",
      "Updating learning rate to 0.000365.\n",
      "Updating learning rate to 0.000355.\n",
      "Updating learning rate to 0.000345.\n",
      "Updating learning rate to 0.000336.\n",
      "Updating learning rate to 0.000326.\n",
      "Updating learning rate to 0.000317.\n",
      "Updating learning rate to 0.000307.\n",
      "Updating learning rate to 0.000298.\n",
      "Updating learning rate to 0.000289.\n",
      "Updating learning rate to 0.000280.\n",
      "Updating learning rate to 0.000271.\n",
      "Updating learning rate to 0.000262.\n",
      "Updating learning rate to 0.000253.\n",
      "Updating learning rate to 0.000244.\n",
      "Updating learning rate to 0.000236.\n",
      "Updating learning rate to 0.000227.\n",
      "Updating learning rate to 0.000219.\n",
      "Updating learning rate to 0.000210.\n",
      "Updating learning rate to 0.000202.\n",
      "Updating learning rate to 0.000194.\n",
      "Updating learning rate to 0.000186.\n",
      "Updating learning rate to 0.000178.\n",
      "Updating learning rate to 0.000170.\n",
      "Updating learning rate to 0.000163.\n",
      "Updating learning rate to 0.000156.\n",
      "Updating learning rate to 0.000148.\n",
      "Updating learning rate to 0.000141.\n",
      "Updating learning rate to 0.000134.\n",
      "Updating learning rate to 0.000127.\n",
      "Updating learning rate to 0.000121.\n",
      "Updating learning rate to 0.000114.\n",
      "Updating learning rate to 0.000108.\n",
      "Updating learning rate to 0.000102.\n",
      "Updating learning rate to 0.000095.\n",
      "Updating learning rate to 0.000090.\n",
      "Updating learning rate to 0.000084.\n",
      "Updating learning rate to 0.000078.\n",
      "Updating learning rate to 0.000073.\n",
      "Updating learning rate to 0.000068.\n",
      "Updating learning rate to 0.000063.\n",
      "Updating learning rate to 0.000058.\n",
      "Updating learning rate to 0.000053.\n",
      "Updating learning rate to 0.000049.\n",
      "Updating learning rate to 0.000045.\n",
      "Updating learning rate to 0.000041.\n",
      "Updating learning rate to 0.000037.\n",
      "Updating learning rate to 0.000033.\n",
      "Updating learning rate to 0.000029.\n",
      "Updating learning rate to 0.000026.\n",
      "Updating learning rate to 0.000023.\n",
      "Updating learning rate to 0.000020.\n",
      "Updating learning rate to 0.000017.\n",
      "Updating learning rate to 0.000015.\n",
      "Updating learning rate to 0.000012.\n",
      "Updating learning rate to 0.000010.\n",
      "Updating learning rate to 0.000008.\n",
      "Updating learning rate to 0.000007.\n",
      "Updating learning rate to 0.000005.\n",
      "Updating learning rate to 0.000004.\n",
      "Updating learning rate to 0.000003.\n",
      "Updating learning rate to 0.000002.\n",
      "Updating learning rate to 0.000001.\n",
      "Updating learning rate to 0.000000.\n",
      "Updating learning rate to 0.000000.\n",
      "Updating learning rate to 0.000000.\n",
      "Epoch 1/1, cost time: 2.98s\n",
      "train loss: 5258062.4415\n",
      "Validation, Loss: 1612330.0000, Metric: -1612330.0000\n",
      "val loss: 1612330.0000, val metric: -1612330.0000\n",
      "Fold 0. Score: -1612330.0\n",
      "length of train dataset: 4935\n",
      "length of val dataset: 4935\n",
      "Updating learning rate to 0.001000.\n",
      "Updating learning rate to 0.001000.\n",
      "Updating learning rate to 0.000999.\n",
      "Updating learning rate to 0.000998.\n",
      "Updating learning rate to 0.000997.\n",
      "Updating learning rate to 0.000996.\n",
      "Updating learning rate to 0.000995.\n",
      "Updating learning rate to 0.000993.\n",
      "Updating learning rate to 0.000992.\n",
      "Updating learning rate to 0.000990.\n",
      "Updating learning rate to 0.000988.\n",
      "Updating learning rate to 0.000985.\n",
      "Updating learning rate to 0.000983.\n",
      "Updating learning rate to 0.000980.\n",
      "Updating learning rate to 0.000977.\n",
      "Updating learning rate to 0.000974.\n",
      "Updating learning rate to 0.000971.\n",
      "Updating learning rate to 0.000967.\n",
      "Updating learning rate to 0.000963.\n",
      "Updating learning rate to 0.000959.\n",
      "Updating learning rate to 0.000955.\n",
      "Updating learning rate to 0.000951.\n",
      "Updating learning rate to 0.000947.\n",
      "Updating learning rate to 0.000942.\n",
      "Updating learning rate to 0.000937.\n",
      "Updating learning rate to 0.000932.\n",
      "Updating learning rate to 0.000927.\n",
      "Updating learning rate to 0.000922.\n",
      "Updating learning rate to 0.000916.\n",
      "Updating learning rate to 0.000910.\n",
      "Updating learning rate to 0.000905.\n",
      "Updating learning rate to 0.000898.\n",
      "Updating learning rate to 0.000892.\n",
      "Updating learning rate to 0.000886.\n",
      "Updating learning rate to 0.000879.\n",
      "Updating learning rate to 0.000873.\n",
      "Updating learning rate to 0.000866.\n",
      "Updating learning rate to 0.000859.\n",
      "Updating learning rate to 0.000852.\n",
      "Updating learning rate to 0.000844.\n",
      "Updating learning rate to 0.000837.\n",
      "Updating learning rate to 0.000830.\n",
      "Updating learning rate to 0.000822.\n",
      "Updating learning rate to 0.000814.\n",
      "Updating learning rate to 0.000806.\n",
      "Updating learning rate to 0.000798.\n",
      "Updating learning rate to 0.000790.\n",
      "Updating learning rate to 0.000781.\n",
      "Updating learning rate to 0.000773.\n",
      "Updating learning rate to 0.000764.\n",
      "Updating learning rate to 0.000756.\n",
      "Updating learning rate to 0.000747.\n",
      "Updating learning rate to 0.000738.\n",
      "Updating learning rate to 0.000729.\n",
      "Updating learning rate to 0.000720.\n",
      "Updating learning rate to 0.000711.\n",
      "Updating learning rate to 0.000702.\n",
      "Updating learning rate to 0.000693.\n",
      "Updating learning rate to 0.000683.\n",
      "Updating learning rate to 0.000674.\n",
      "Updating learning rate to 0.000664.\n",
      "Updating learning rate to 0.000655.\n",
      "Updating learning rate to 0.000645.\n",
      "Updating learning rate to 0.000635.\n",
      "Updating learning rate to 0.000625.\n",
      "Updating learning rate to 0.000615.\n",
      "Updating learning rate to 0.000606.\n",
      "Updating learning rate to 0.000596.\n",
      "Updating learning rate to 0.000586.\n",
      "Updating learning rate to 0.000576.\n",
      "Updating learning rate to 0.000566.\n",
      "Updating learning rate to 0.000556.\n",
      "Updating learning rate to 0.000546.\n",
      "Updating learning rate to 0.000535.\n",
      "Updating learning rate to 0.000525.\n",
      "Updating learning rate to 0.000515.\n",
      "Updating learning rate to 0.000505.\n",
      "Updating learning rate to 0.000495.\n",
      "Updating learning rate to 0.000485.\n",
      "Updating learning rate to 0.000475.\n",
      "Updating learning rate to 0.000465.\n",
      "Updating learning rate to 0.000454.\n",
      "Updating learning rate to 0.000444.\n",
      "Updating learning rate to 0.000434.\n",
      "Updating learning rate to 0.000424.\n",
      "Updating learning rate to 0.000414.\n",
      "Updating learning rate to 0.000404.\n",
      "Updating learning rate to 0.000394.\n",
      "Updating learning rate to 0.000385.\n",
      "Updating learning rate to 0.000375.\n",
      "Updating learning rate to 0.000365.\n",
      "Updating learning rate to 0.000355.\n",
      "Updating learning rate to 0.000345.\n",
      "Updating learning rate to 0.000336.\n",
      "Updating learning rate to 0.000326.\n",
      "Updating learning rate to 0.000317.\n",
      "Updating learning rate to 0.000307.\n",
      "Updating learning rate to 0.000298.\n",
      "Updating learning rate to 0.000289.\n",
      "Updating learning rate to 0.000280.\n",
      "Updating learning rate to 0.000271.\n",
      "Updating learning rate to 0.000262.\n",
      "Updating learning rate to 0.000253.\n",
      "Updating learning rate to 0.000244.\n",
      "Updating learning rate to 0.000236.\n",
      "Updating learning rate to 0.000227.\n",
      "Updating learning rate to 0.000219.\n",
      "Updating learning rate to 0.000210.\n",
      "Updating learning rate to 0.000202.\n",
      "Updating learning rate to 0.000194.\n",
      "Updating learning rate to 0.000186.\n",
      "Updating learning rate to 0.000178.\n",
      "Updating learning rate to 0.000170.\n",
      "Updating learning rate to 0.000163.\n",
      "Updating learning rate to 0.000156.\n",
      "Updating learning rate to 0.000148.\n",
      "Updating learning rate to 0.000141.\n",
      "Updating learning rate to 0.000134.\n",
      "Updating learning rate to 0.000127.\n",
      "Updating learning rate to 0.000121.\n",
      "Updating learning rate to 0.000114.\n",
      "Updating learning rate to 0.000108.\n",
      "Updating learning rate to 0.000102.\n",
      "Updating learning rate to 0.000095.\n",
      "Updating learning rate to 0.000090.\n",
      "Updating learning rate to 0.000084.\n",
      "Updating learning rate to 0.000078.\n",
      "Updating learning rate to 0.000073.\n",
      "Updating learning rate to 0.000068.\n",
      "Updating learning rate to 0.000063.\n",
      "Updating learning rate to 0.000058.\n",
      "Updating learning rate to 0.000053.\n",
      "Updating learning rate to 0.000049.\n",
      "Updating learning rate to 0.000045.\n",
      "Updating learning rate to 0.000041.\n",
      "Updating learning rate to 0.000037.\n",
      "Updating learning rate to 0.000033.\n",
      "Updating learning rate to 0.000029.\n",
      "Updating learning rate to 0.000026.\n",
      "Updating learning rate to 0.000023.\n",
      "Updating learning rate to 0.000020.\n",
      "Updating learning rate to 0.000017.\n",
      "Updating learning rate to 0.000015.\n",
      "Updating learning rate to 0.000012.\n",
      "Updating learning rate to 0.000010.\n",
      "Updating learning rate to 0.000008.\n",
      "Updating learning rate to 0.000007.\n",
      "Updating learning rate to 0.000005.\n",
      "Updating learning rate to 0.000004.\n",
      "Updating learning rate to 0.000003.\n",
      "Updating learning rate to 0.000002.\n",
      "Updating learning rate to 0.000001.\n",
      "Updating learning rate to 0.000000.\n",
      "Updating learning rate to 0.000000.\n",
      "Updating learning rate to 0.000000.\n",
      "Epoch 1/1, cost time: 3.05s\n",
      "train loss: 5405654.7412\n",
      "Validation, Loss: 1730647.8750, Metric: -1730647.8750\n",
      "val loss: 1730647.8750, val metric: -1730647.8750\n",
      "Fold 1. Score: -1730647.875\n",
      "Mean score: -1671488.9375\n",
      "Std: 59158.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11.653326034545898, <tsururu.strategies.mimo.MIMOStrategy at 0x1621f3cb0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq: Day; period: 1\n",
      "length of test dataset: 10\n"
     ]
    }
   ],
   "source": [
    "forecast_time, current_pred = strategy.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>3599.11377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>3599.426025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>3599.793945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>3600.220215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>3600.378174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>9</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>8667.804688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>9</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>8668.47168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>9</td>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>8668.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>9</td>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>8669.640625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>9</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>8670.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       date        value\n",
       "0   0 2022-09-27   3599.11377\n",
       "1   0 2022-09-28  3599.426025\n",
       "2   0 2022-09-29  3599.793945\n",
       "3   0 2022-09-30  3600.220215\n",
       "4   0 2022-10-01  3600.378174\n",
       ".. ..        ...          ...\n",
       "65  9 2022-09-29  8667.804688\n",
       "66  9 2022-09-30   8668.47168\n",
       "67  9 2022-10-01  8668.984375\n",
       "68  9 2022-10-02  8669.640625\n",
       "69  9 2022-10-03      8670.25\n",
       "\n",
       "[70 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and loading checkpoints is an essential practice in training DL models. \n",
    "\n",
    "Let's explore how to save checkpoints to disk, what structure the saved files have, and how to restore the model from a checkpoint for either fine-tuning or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Save and load checkpoints](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s consider working with checkpoints using the Direct strategy as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_params = {\n",
    "    \"device\": \"cpu\",\n",
    "    \"num_workers\": 0,\n",
    "    \"best_by_metric\": True,\n",
    "    # Let's enable save_to_dir (by the way, default value is True)\n",
    "    \"save_to_dir\": True,\n",
    "    \"checkpoint_path\": \"checkpoints/\",\n",
    "    # Save checkpoints for 3 best model\n",
    "    \"save_k_best\": 3,\n",
    "    # Average checkpoints for the final model\n",
    "    \"average_snapshots\": True,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "trainer = DLTrainer(\n",
    "    model, \n",
    "    model_params, \n",
    "    validation, \n",
    "    validation_params, \n",
    "    **trainer_params\n",
    ")\n",
    "\n",
    "strategy = MIMOStrategy(\n",
    "    pipeline=pipeline,\n",
    "    trainer=trainer,\n",
    "    horizon=horizon,\n",
    "    history=history,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[Save checkpoint](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train dataset: 4935\n",
      "length of val dataset: 4935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, cost time: 2.65s\n",
      "train loss: 4887171.9910\n",
      "Validation, Loss: 1047792.3750, Metric: -1047792.3750\n",
      "val loss: 1047792.3750, val metric: -1047792.3750\n",
      "Epoch 2/10, cost time: 2.48s\n",
      "train loss: 706722.3347\n",
      "Validation, Loss: 462100.9062, Metric: -462100.9062\n",
      "val loss: 462100.9062, val metric: -462100.9062\n",
      "Epoch 3/10, cost time: 2.46s\n",
      "train loss: 276882.1953\n",
      "Validation, Loss: 154248.0312, Metric: -154248.0312\n",
      "val loss: 154248.0312, val metric: -154248.0312\n",
      "Epoch 4/10, cost time: 2.48s\n",
      "train loss: 83515.5311\n",
      "Validation, Loss: 39639.5547, Metric: -39639.5547\n",
      "val loss: 39639.5547, val metric: -39639.5547\n",
      "Epoch 5/10, cost time: 2.45s\n",
      "train loss: 19590.2556\n",
      "Validation, Loss: 8090.8174, Metric: -8090.8174\n",
      "val loss: 8090.8174, val metric: -8090.8174\n",
      "Epoch 6/10, cost time: 3.00s\n",
      "train loss: 3694.6669\n",
      "Validation, Loss: 1400.1285, Metric: -1400.1285\n",
      "val loss: 1400.1285, val metric: -1400.1285\n",
      "Epoch 7/10, cost time: 3.29s\n",
      "train loss: 635.7974\n",
      "Validation, Loss: 266.9762, Metric: -266.9762\n",
      "val loss: 266.9762, val metric: -266.9762\n",
      "Epoch 8/10, cost time: 2.93s\n",
      "train loss: 150.1327\n",
      "Validation, Loss: 89.3946, Metric: -89.3946\n",
      "val loss: 89.3946, val metric: -89.3946\n",
      "Epoch 9/10, cost time: 2.74s\n",
      "train loss: 62.6007\n",
      "Validation, Loss: 44.3605, Metric: -44.3605\n",
      "val loss: 44.3605, val metric: -44.3605\n",
      "Epoch 10/10, cost time: 2.84s\n",
      "train loss: 32.5973\n",
      "Validation, Loss: 23.0942, Metric: -23.0942\n",
      "val loss: 23.0942, val metric: -23.0942\n",
      "Fold 0. Score: -23.094181060791016\n",
      "length of train dataset: 4935\n",
      "length of val dataset: 4935\n",
      "Epoch 1/10, cost time: 2.82s\n",
      "train loss: 4629491.8015\n",
      "Validation, Loss: 983696.3750, Metric: -983696.3750\n",
      "val loss: 983696.3750, val metric: -983696.3750\n",
      "Epoch 2/10, cost time: 2.53s\n",
      "train loss: 698127.7240\n",
      "Validation, Loss: 403440.4688, Metric: -403440.4688\n",
      "val loss: 403440.4688, val metric: -403440.4688\n",
      "Epoch 3/10, cost time: 2.52s\n",
      "train loss: 250679.8958\n",
      "Validation, Loss: 121161.9219, Metric: -121161.9219\n",
      "val loss: 121161.9219, val metric: -121161.9219\n",
      "Epoch 4/10, cost time: 2.52s\n",
      "train loss: 67178.7610\n",
      "Validation, Loss: 27191.2715, Metric: -27191.2715\n",
      "val loss: 27191.2715, val metric: -27191.2715\n",
      "Epoch 5/10, cost time: 2.53s\n",
      "train loss: 13710.6542\n",
      "Validation, Loss: 4644.9053, Metric: -4644.9053\n",
      "val loss: 4644.9053, val metric: -4644.9053\n",
      "Epoch 6/10, cost time: 2.51s\n",
      "train loss: 2181.1388\n",
      "Validation, Loss: 668.1273, Metric: -668.1273\n",
      "val loss: 668.1273, val metric: -668.1273\n",
      "Epoch 7/10, cost time: 2.53s\n",
      "train loss: 317.9138\n",
      "Validation, Loss: 117.8485, Metric: -117.8485\n",
      "val loss: 117.8485, val metric: -117.8485\n",
      "Epoch 8/10, cost time: 2.54s\n",
      "train loss: 73.0912\n",
      "Validation, Loss: 43.1988, Metric: -43.1988\n",
      "val loss: 43.1988, val metric: -43.1988\n",
      "Epoch 9/10, cost time: 2.50s\n",
      "train loss: 31.8865\n",
      "Validation, Loss: 22.1970, Metric: -22.1970\n",
      "val loss: 22.1970, val metric: -22.1970\n",
      "Epoch 10/10, cost time: 2.51s\n",
      "train loss: 16.3420\n",
      "Validation, Loss: 11.1961, Metric: -11.1961\n",
      "val loss: 11.1961, val metric: -11.1961\n",
      "Fold 1. Score: -11.196077346801758\n",
      "Mean score: -17.1451\n",
      "Std: 5.9491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(104.16126585006714, <tsururu.strategies.mimo.MIMOStrategy at 0x168f70530>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Load checkpoint for finetune](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the saved checkpoints, we can continue training by passing the pretrained path and another checkpoint path to the trainerâ€™s parameters. All other parameters remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_params = {\n",
    "    \"device\": \"cpu\",\n",
    "    \"num_workers\": 0,\n",
    "    \"best_by_metric\": True,\n",
    "    # Let's enable save_to_dir (by the way, default value is True)\n",
    "    \"save_to_dir\": True,\n",
    "    \"pretrained_path\": \"checkpoints/\",\n",
    "    \"checkpoint_path\": \"checkpoints_finetuned/\",\n",
    "    # Save checkpoints for 3 best model\n",
    "    \"save_k_best\": 3,\n",
    "    # Average checkpoints for the final model\n",
    "    \"average_snapshots\": True,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "trainer = DLTrainer(\n",
    "    model, \n",
    "    model_params, \n",
    "    validation, \n",
    "    validation_params, \n",
    "    **trainer_params\n",
    ")\n",
    "\n",
    "strategy = MIMOStrategy(\n",
    "    pipeline=pipeline,\n",
    "    trainer=trainer,\n",
    "    horizon=horizon,\n",
    "    history=history,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train dataset: 4935\n",
      "length of val dataset: 4935\n",
      "Epoch 1/10, cost time: 2.58s\n",
      "train loss: 16.7105\n",
      "Validation, Loss: 11.4393, Metric: -11.4393\n",
      "val loss: 11.4393, val metric: -11.4393\n",
      "Epoch 2/10, cost time: 2.50s\n",
      "train loss: 8.0148\n",
      "Validation, Loss: 5.2676, Metric: -5.2676\n",
      "val loss: 5.2676, val metric: -5.2676\n",
      "Epoch 3/10, cost time: 2.49s\n",
      "train loss: 3.5997\n",
      "Validation, Loss: 2.2890, Metric: -2.2890\n",
      "val loss: 2.2890, val metric: -2.2890\n",
      "Epoch 4/10, cost time: 2.48s\n",
      "train loss: 1.4931\n",
      "Validation, Loss: 0.9049, Metric: -0.9049\n",
      "val loss: 0.9049, val metric: -0.9049\n",
      "Epoch 5/10, cost time: 2.50s\n",
      "train loss: 0.5777\n",
      "Validation, Loss: 0.3394, Metric: -0.3394\n",
      "val loss: 0.3394, val metric: -0.3394\n",
      "Epoch 6/10, cost time: 2.50s\n",
      "train loss: 0.2106\n",
      "Validation, Loss: 0.1200, Metric: -0.1200\n",
      "val loss: 0.1200, val metric: -0.1200\n",
      "Epoch 7/10, cost time: 2.49s\n",
      "train loss: 0.0762\n",
      "Validation, Loss: 0.0456, Metric: -0.0456\n",
      "val loss: 0.0456, val metric: -0.0456\n",
      "Epoch 8/10, cost time: 2.49s\n",
      "train loss: 0.0317\n",
      "Validation, Loss: 0.0217, Metric: -0.0217\n",
      "val loss: 0.0217, val metric: -0.0217\n",
      "Epoch 9/10, cost time: 2.49s\n",
      "train loss: 0.0179\n",
      "Validation, Loss: 0.0148, Metric: -0.0148\n",
      "val loss: 0.0148, val metric: -0.0148\n",
      "Epoch 10/10, cost time: 2.49s\n",
      "train loss: 0.0136\n",
      "Validation, Loss: 0.0123, Metric: -0.0123\n",
      "val loss: 0.0123, val metric: -0.0123\n",
      "Fold 0. Score: -0.012287751771509647\n",
      "length of train dataset: 4935\n",
      "length of val dataset: 4935\n",
      "Epoch 1/10, cost time: 2.49s\n",
      "train loss: 8.0399\n",
      "Validation, Loss: 5.3132, Metric: -5.3132\n",
      "val loss: 5.3132, val metric: -5.3132\n",
      "Epoch 2/10, cost time: 2.48s\n",
      "train loss: 3.6823\n",
      "Validation, Loss: 2.3280, Metric: -2.3280\n",
      "val loss: 2.3280, val metric: -2.3280\n",
      "Epoch 3/10, cost time: 2.50s\n",
      "train loss: 1.5790\n",
      "Validation, Loss: 0.9588, Metric: -0.9588\n",
      "val loss: 0.9588, val metric: -0.9588\n",
      "Epoch 4/10, cost time: 2.49s\n",
      "train loss: 0.6296\n",
      "Validation, Loss: 0.3646, Metric: -0.3646\n",
      "val loss: 0.3646, val metric: -0.3646\n",
      "Epoch 5/10, cost time: 2.51s\n",
      "train loss: 0.2345\n",
      "Validation, Loss: 0.1344, Metric: -0.1344\n",
      "val loss: 0.1344, val metric: -0.1344\n",
      "Epoch 6/10, cost time: 2.69s\n",
      "train loss: 0.0874\n",
      "Validation, Loss: 0.0521, Metric: -0.0521\n",
      "val loss: 0.0521, val metric: -0.0521\n",
      "Epoch 7/10, cost time: 2.49s\n",
      "train loss: 0.0368\n",
      "Validation, Loss: 0.0258, Metric: -0.0258\n",
      "val loss: 0.0258, val metric: -0.0258\n",
      "Epoch 8/10, cost time: 2.48s\n",
      "train loss: 0.0204\n",
      "Validation, Loss: 0.0171, Metric: -0.0171\n",
      "val loss: 0.0171, val metric: -0.0171\n",
      "Epoch 9/10, cost time: 2.51s\n",
      "train loss: 0.0152\n",
      "Validation, Loss: 0.0141, Metric: -0.0141\n",
      "val loss: 0.0141, val metric: -0.0141\n",
      "Epoch 10/10, cost time: 2.50s\n",
      "train loss: 0.0132\n",
      "Validation, Loss: 0.0127, Metric: -0.0127\n",
      "val loss: 0.0127, val metric: -0.0127\n",
      "Fold 1. Score: -0.012707843445241451\n",
      "Mean score: -0.0125\n",
      "Std: 0.0002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99.83380794525146, <tsururu.strategies.mimo.MIMOStrategy at 0x168b8ca70>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[Load checkpoint for inference](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_params = {\n",
    "    \"device\": \"cpu\",\n",
    "    \"num_workers\": 0,\n",
    "    \"n_epochs\": 1,\n",
    "    \"pretrained_path\": \"checkpoints_finetuned/\",\n",
    "    # Average checkpoints for the final model\n",
    "    \"average_snapshots\": True,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "trainer = DLTrainer(\n",
    "    model, \n",
    "    model_params, \n",
    "    validation, \n",
    "    validation_params, \n",
    "    **trainer_params\n",
    ")\n",
    "\n",
    "strategy = MIMOStrategy(\n",
    "    pipeline=pipeline,\n",
    "    trainer=trainer,\n",
    "    horizon=horizon,\n",
    "    history=history,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train dataset: 4935\n",
      "length of val dataset: 4935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, cost time: 2.55s\n",
      "train loss: 0.0120\n",
      "Validation, Loss: 0.0112, Metric: -0.0112\n",
      "val loss: 0.0112, val metric: -0.0112\n",
      "Fold 0. Score: -0.011183073744177818\n",
      "length of train dataset: 4935\n",
      "length of val dataset: 4935\n",
      "Epoch 1/1, cost time: 2.49s\n",
      "train loss: 0.0120\n",
      "Validation, Loss: 0.0117, Metric: -0.0117\n",
      "val loss: 0.0117, val metric: -0.0117\n",
      "Fold 1. Score: -0.011746459640562534\n",
      "Mean score: -0.0115\n",
      "Std: 0.0003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10.000695943832397, <tsururu.strategies.mimo.MIMOStrategy at 0x102fd17c0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq: Day; period: 1\n",
      "length of test dataset: 10\n"
     ]
    }
   ],
   "source": [
    "forecast_time, current_pred = strategy.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>1999.970093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>2000.972412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>2001.976074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>2002.978027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>2003.981201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>9</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>11001.958984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>9</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>11002.960938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>9</td>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>11003.962891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>9</td>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>11004.966797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>9</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>11005.970703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       date         value\n",
       "0   0 2022-09-27   1999.970093\n",
       "1   0 2022-09-28   2000.972412\n",
       "2   0 2022-09-29   2001.976074\n",
       "3   0 2022-09-30   2002.978027\n",
       "4   0 2022-10-01   2003.981201\n",
       ".. ..        ...           ...\n",
       "65  9 2022-09-29  11001.958984\n",
       "66  9 2022-09-30  11002.960938\n",
       "67  9 2022-10-01  11003.962891\n",
       "68  9 2022-10-02  11004.966797\n",
       "69  9 2022-10-03  11005.970703\n",
       "\n",
       "[70 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tsururu-dev)",
   "language": "python",
   "name": "tsururu-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
