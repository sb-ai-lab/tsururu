{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tsururu.dataset import Pipeline, TSDataset\n",
    "from tsururu.model_training.trainer import DLTrainer\n",
    "from tsururu.model_training.validator import HoldOutValidator\n",
    "from tsururu.models.torch_based.dlinear import DLinear_NN\n",
    "from tsururu.strategies import MIMOStrategy\n",
    "from tsururu.transformers import (\n",
    "    LagTransformer,\n",
    "    SequentialTransformer,\n",
    "    TargetGenerator,\n",
    "    UnionTransformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(\n",
    "    cv: int,\n",
    "    regime: str,\n",
    "    y_true: Optional[List[np.ndarray]] = None,\n",
    "    y_pred: Optional[List[np.ndarray]] = None,\n",
    "    ids: Optional[List[Union[float, str]]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    def _get_fold_value(\n",
    "        value: Optional[Union[float, np.ndarray]], idx: int\n",
    "    ) -> List[Optional[Union[float, np.ndarray]]]:\n",
    "        if value is None:\n",
    "            return [None]\n",
    "        if isinstance(value[idx], float):\n",
    "            return value[idx]\n",
    "        if isinstance(value[idx], np.ndarray):\n",
    "            return value[idx].reshape(-1)\n",
    "        raise TypeError(f\"Unexpected value type. Value: {value}\")\n",
    "\n",
    "    df_res_dict = {}\n",
    "\n",
    "    for idx_fold in range(cv):\n",
    "        # Fill df_res_dict\n",
    "        for name, value in [(\"y_true\", y_true), (\"y_pred\", y_pred)]:\n",
    "            df_res_dict[f\"{name}_{idx_fold+1}\"] = _get_fold_value(value, idx_fold)\n",
    "        if regime != \"local\":\n",
    "            df_res_dict[f\"id_{idx_fold+1}\"] = _get_fold_value(ids, idx_fold)\n",
    "\n",
    "    # Save datasets to specified directory\n",
    "    df_res = pd.DataFrame(df_res_dict)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_val_with_train(train_data, val_data, id_column, date_column, history):\n",
    "    L_split_data = train_data[date_column].values[(len(train_data) - history)]\n",
    "    L_last_train_data = train_data[train_data[date_column] >= L_split_data]\n",
    "    val_data_expanded = pd.concat((L_last_train_data, val_data))\n",
    "    val_data_expanded = val_data_expanded.sort_values([id_column, date_column]).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    return val_data_expanded\n",
    "\n",
    "\n",
    "def expand_test_with_val_and_train(\n",
    "    train_data, val_data, test_data, id_column, date_column, history\n",
    "):\n",
    "    unqiue_id_cnt = val_data[id_column].nunique()\n",
    "    L_split_data = val_data[date_column].values[\n",
    "        (\n",
    "            (len(val_data) - history)\n",
    "            if (len(val_data) // val_data[id_column].nunique() - history) > 0\n",
    "            else 0\n",
    "        )\n",
    "    ]\n",
    "    L_last_val_data = val_data[val_data[date_column] >= L_split_data]\n",
    "    if len(val_data) // unqiue_id_cnt - history < 0:\n",
    "        if (len(train_data) - (history - len(L_last_val_data) / unqiue_id_cnt)) > 0:\n",
    "            L_split_data = train_data[date_column].values[\n",
    "                (\n",
    "                    len(train_data) // unqiue_id_cnt\n",
    "                    - (history - len(L_last_val_data) // unqiue_id_cnt)\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            L_split_data = 0\n",
    "        L_last_train_data = train_data[train_data[date_column] >= L_split_data]\n",
    "        test_data_expanded = pd.concat((L_last_train_data, L_last_val_data, test_data))\n",
    "    else:\n",
    "        test_data_expanded = pd.concat((L_last_val_data, test_data))\n",
    "    test_data_expanded = test_data_expanded.sort_values([id_column, date_column]).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    return test_data_expanded\n",
    "\n",
    "\n",
    "def get_train_val_test_datasets(dataset_path, columns_params, train_size, test_size, history):\n",
    "    data = pd.read_csv(dataset_path)\n",
    "\n",
    "    date_column = columns_params[\"date\"][\"columns\"][0]\n",
    "    id_column = columns_params[\"id\"][\"columns\"][0]\n",
    "\n",
    "    if dataset_path.parts[-1] in [\"ETTh1.csv\", \"ETTh2.csv\", \"ETTm1.csv\", \"ETTm2.csv\"]:\n",
    "        train_val_split_data = \"2017-06-25 23:00:00\"\n",
    "        val_test_slit_data = \"2017-10-23 23:00:00\"\n",
    "    else:\n",
    "        train_val_split_data = data[date_column].values[\n",
    "            int(data[date_column].nunique() * train_size)\n",
    "        ]\n",
    "        val_test_slit_data = data[date_column].values[\n",
    "            int(data[date_column].nunique() * (1 - test_size))\n",
    "        ]\n",
    "\n",
    "    train_data = data[data[date_column] <= train_val_split_data]\n",
    "    val_data = data[\n",
    "        (data[date_column] > train_val_split_data) & (data[date_column] <= val_test_slit_data)\n",
    "    ]\n",
    "    test_data = data[data[date_column] > val_test_slit_data]\n",
    "    val_data = expand_val_with_train(train_data, val_data, id_column, date_column, history)\n",
    "    test_data_expanded = expand_test_with_val_and_train(\n",
    "        train_data, val_data, test_data, id_column, date_column, history\n",
    "    )\n",
    "\n",
    "    # train, val and test TSDataset initialization\n",
    "    train_dataset = TSDataset(\n",
    "        data=train_data,\n",
    "        columns_params=columns_params,\n",
    "    )\n",
    "\n",
    "    val_dataset = TSDataset(\n",
    "        data=val_data,\n",
    "        columns_params=columns_params,\n",
    "    )\n",
    "\n",
    "    test_dataset = TSDataset(\n",
    "        data=test_data_expanded,\n",
    "        columns_params=columns_params,\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize TSDataset, Pipeline, Model, Validator, Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.7\n",
    "TEST_SIZE = 0.2\n",
    "history = 7\n",
    "\n",
    "df_path = Path(\"datasets/global/simulated_data_to_check.csv\")\n",
    "\n",
    "columns_params = {\n",
    "    \"target\": {\n",
    "        \"columns\": [\"value\"],\n",
    "        \"type\": \"continious\",\n",
    "    },\n",
    "    \"date\": {\n",
    "        \"columns\": [\"date\"],\n",
    "        \"type\": \"datetime\",\n",
    "    },\n",
    "    \"id\": {\n",
    "        \"columns\": [\"id\"],\n",
    "        \"type\": \"categorical\",\n",
    "    }\n",
    "}\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = get_train_val_test_datasets(\n",
    "    df_path, columns_params, TRAIN_SIZE, TEST_SIZE, history\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = LagTransformer(lags=7)\n",
    "target_generator = TargetGenerator()\n",
    "\n",
    "union_1 = UnionTransformer(transformers_list=[lag, target_generator])\n",
    "seq_1 = SequentialTransformer(transformers_list=[union_1], input_features=[\"value\"])\n",
    "union = UnionTransformer(transformers_list=[seq_1])\n",
    "\n",
    "pipeline = Pipeline(union, multivariate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model parameters\n",
    "model = DLinear_NN\n",
    "model_params = {\"moving_avg\": 7, \"individual\": False, \"enc_in\": None}\n",
    "\n",
    "validation = HoldOutValidator\n",
    "validation_params = {\"validation_data\": val_dataset}\n",
    "\n",
    "trainer_params = {\n",
    "    \"device\": \"cpu\",\n",
    "    \"num_workers\": 0,\n",
    "    \"best_by_metric\": True,\n",
    "}\n",
    "\n",
    "trainer = DLTrainer(\n",
    "    model, \n",
    "    model_params, \n",
    "    validation, \n",
    "    validation_params, \n",
    "    **trainer_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 7\n",
    "model_horizon = 7\n",
    "history = 7\n",
    "step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = MIMOStrategy(\n",
    "    pipeline=pipeline,\n",
    "    trainer=trainer,\n",
    "    horizon=horizon,\n",
    "    history=history,\n",
    "    step=step,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train dataset: 688\n",
      "length of val dataset: 94\n",
      "Epoch 1/10, cost time: 0.77s\n",
      "train loss: 274.1573\n",
      "Validation, Loss: 30.1381, Metric: -30.1381\n",
      "val loss: 30.1381\n",
      "Model saved to checkpoints/fold_0/model_0.pth\n",
      "Optimizer saved to checkpoints/fold_0/opt_0.pth\n",
      "Epoch 2/10, cost time: 0.72s\n",
      "train loss: 32.8188\n",
      "Validation, Loss: 14.3539, Metric: -14.3539\n",
      "val loss: 14.3539\n",
      "Model saved to checkpoints/fold_0/model_1.pth\n",
      "Optimizer saved to checkpoints/fold_0/opt_1.pth\n",
      "Epoch 3/10, cost time: 0.72s\n",
      "train loss: 12.8709\n",
      "Validation, Loss: 10.0110, Metric: -10.0110\n",
      "val loss: 10.0110\n",
      "Model saved to checkpoints/fold_0/model_2.pth\n",
      "Optimizer saved to checkpoints/fold_0/opt_2.pth\n",
      "Epoch 4/10, cost time: 0.73s\n",
      "train loss: 10.2632\n",
      "Validation, Loss: 8.9626, Metric: -8.9626\n",
      "val loss: 8.9626\n",
      "Model saved to checkpoints/fold_0/model_3.pth\n",
      "Optimizer saved to checkpoints/fold_0/opt_3.pth\n",
      "Epoch 5/10, cost time: 0.72s\n",
      "train loss: 9.9284\n",
      "Validation, Loss: 8.8743, Metric: -8.8743\n",
      "val loss: 8.8743\n",
      "Model saved to checkpoints/fold_0/model_4.pth\n",
      "Optimizer saved to checkpoints/fold_0/opt_4.pth\n",
      "Epoch 6/10, cost time: 0.72s\n",
      "train loss: 9.8400\n",
      "Validation, Loss: 8.8730, Metric: -8.8730\n",
      "val loss: 8.8730\n",
      "Removing worst model snapshot: from epoch 0\n",
      "Model saved to checkpoints/fold_0/model_5.pth\n",
      "Optimizer saved to checkpoints/fold_0/opt_5.pth\n",
      "Epoch 7/10, cost time: 0.72s\n",
      "train loss: 9.7701\n",
      "Validation, Loss: 8.8641, Metric: -8.8641\n",
      "val loss: 8.8641\n",
      "Removing worst model snapshot: from epoch 1\n",
      "Model saved to checkpoints/fold_0/model_6.pth\n",
      "Optimizer saved to checkpoints/fold_0/opt_6.pth\n",
      "Epoch 8/10, cost time: 0.72s\n",
      "train loss: 9.7031\n",
      "Validation, Loss: 8.6850, Metric: -8.6850\n",
      "val loss: 8.6850\n",
      "Removing worst model snapshot: from epoch 2\n",
      "Model saved to checkpoints/fold_0/model_7.pth\n",
      "Optimizer saved to checkpoints/fold_0/opt_7.pth\n",
      "Epoch 9/10, cost time: 0.75s\n",
      "train loss: 9.6208\n",
      "Validation, Loss: 8.6171, Metric: -8.6171\n",
      "val loss: 8.6171\n",
      "Removing worst model snapshot: from epoch 3\n",
      "Model saved to checkpoints/fold_0/model_8.pth\n",
      "Optimizer saved to checkpoints/fold_0/opt_8.pth\n",
      "Epoch 10/10, cost time: 0.83s\n",
      "train loss: 9.5413\n",
      "Validation, Loss: 8.5182, Metric: -8.5182\n",
      "val loss: 8.5182\n",
      "Removing worst model snapshot: from epoch 4\n",
      "Model saved to checkpoints/fold_0/model_9.pth\n",
      "Optimizer saved to checkpoints/fold_0/opt_9.pth\n",
      "Fold 0. Score: -8.518207550048828\n",
      "Mean score: -8.5182\n",
      "Std: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8.324044942855835, <tsururu.strategies.mimo.MIMOStrategy at 0x302253fd0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of test dataset: 193\n",
      "Validation, Loss: nan, Metric: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6019949913024902,\n",
       "       id       date         value\n",
       " 0      0 2022-03-12   1798.098267\n",
       " 1      0 2022-03-13   1798.139038\n",
       " 2      0 2022-03-14   1798.520996\n",
       " 3      0 2022-03-15   1798.796387\n",
       " 4      0 2022-03-16   1799.245605\n",
       " ...   ..        ...           ...\n",
       " 13505  9 2022-09-22  10997.950195\n",
       " 13506  9 2022-09-23  10999.390625\n",
       " 13507  9 2022-09-24  11000.709961\n",
       " 13508  9 2022-09-25  11002.036133\n",
       " 13509  9 2022-09-26  11003.500977\n",
       " \n",
       " [13510 rows x 3 columns])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy.predict(test_dataset, test_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
