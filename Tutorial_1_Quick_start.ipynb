{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore a basic example of forecasting multiple time series and go over the key components of the forecasting pipeline provided by the `Tsururu` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import everything we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import polars as pl\n",
    "\n",
    "from tsururu.dataset import Pipeline, TSDataset, TSDatasetPolars, TSDatasetNumba, TSDatasetNumbaPolars\n",
    "from tsururu.model_training.trainer import MLTrainer\n",
    "from tsururu.model_training.validator import KFoldCrossValidator\n",
    "from tsururu.models.boost import CatBoost\n",
    "from tsururu.strategies import RecursiveStrategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(\n",
    "    cv: int,\n",
    "    regime: str,\n",
    "    y_true: Optional[List[np.ndarray]] = None,\n",
    "    y_pred: Optional[List[np.ndarray]] = None,\n",
    "    ids: Optional[List[Union[float, str]]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    def _get_fold_value(\n",
    "        value: Optional[Union[float, np.ndarray]], idx: int\n",
    "    ) -> List[Optional[Union[float, np.ndarray]]]:\n",
    "        if value is None:\n",
    "            return [None]\n",
    "        if isinstance(value[idx], float):\n",
    "            return value[idx]\n",
    "        if isinstance(value[idx], np.ndarray):\n",
    "            return value[idx].reshape(-1)\n",
    "        raise TypeError(f\"Unexpected value type. Value: {value}\")\n",
    "\n",
    "    df_res_dict = {}\n",
    "\n",
    "    for idx_fold in range(cv):\n",
    "        # Fill df_res_dict\n",
    "        for name, value in [(\"y_true\", y_true), (\"y_pred\", y_pred)]:\n",
    "            df_res_dict[f\"{name}_{idx_fold+1}\"] = _get_fold_value(\n",
    "                value, idx_fold\n",
    "            )\n",
    "        if regime != \"local\":\n",
    "            df_res_dict[f\"id_{idx_fold+1}\"] = _get_fold_value(ids, idx_fold)\n",
    "\n",
    "    # Save datasets to specified directory\n",
    "    df_res = pd.DataFrame(df_res_dict)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several main objects to look out for when working with the library:\n",
    "1) `TSDataset`.\n",
    "2) `Pipeline` and `Transformers`\n",
    "3) `Strategy`.\n",
    "4) `Model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is needed to store data and meta-information about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialise it is necessary to submit the data in `pd.DataFrame` format and define some meta-information about roles that necessary for solving the task of time series forecasting: `id`, `date`, `target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = \"datasets/global/simulated_data_to_check.csv\"\n",
    "\n",
    "dataset_params = {\n",
    "    \"target\": {\n",
    "        \"columns\": [\"value\"],\n",
    "        \"type\": \"continious\",\n",
    "    },\n",
    "    \"date\": {\n",
    "        \"columns\": [\"date\"],\n",
    "        \"type\": \"datetime\",\n",
    "    },\n",
    "    \"id\": {\n",
    "        \"columns\": [\"id\"],\n",
    "        \"type\": \"categorical\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq: Day; period: 1\n",
      "0.02325606346130371\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dataset = TSDataset(\n",
    "    data=pd.read_csv(df_path),\n",
    "    columns_params=dataset_params,\n",
    "    print_freq_period_info=True,\n",
    ")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq: Day; period: 1\n",
      "0.048441171646118164\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dataset = TSDatasetPolars(\n",
    "    data=pl.read_csv(df_path),\n",
    "    columns_params=dataset_params,\n",
    "    print_freq_period_info=True,\n",
    ")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq: Day; period: 1\n",
      "0.02024221420288086\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dataset = TSDatasetNumba(\n",
    "    data=pd.read_csv(df_path),\n",
    "    columns_params=dataset_params,\n",
    "    print_freq_period_info=True,\n",
    ")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq: Day; period: 1\n",
      "0.01135706901550293\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dataset = TSDatasetNumbaPolars(\n",
    "    data=pl.read_csv(df_path),\n",
    "    columns_params=dataset_params,\n",
    "    print_freq_period_info=True,\n",
    ")\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pipeline class is designed to create and apply a sequence of transformations (transformers) to time series data. It is used for data preprocessing, feature and target generation, as well as performing transformations required for forecasting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will cover a simplified approach to initializing the Pipeline. For detailed information on the available transformers and methods for building a pipeline, refer to Tutorial 3 (Tutorial_3_Pipeline.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What kind of transformers are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special attention should be paid to the `Transformer` class: the elements of the pipeline that are responsible for transforming the values of a series and generating features. `Pipeline` class is a wrapper over transformers which is needed to provide some additional methods and functions above them.\n",
    "\n",
    "There are two types of transformers that are used to collect pipelines:\n",
    "- `Union` transformers;\n",
    "- `Sequential` transformers.\n",
    "\n",
    "Below is a list of available Transformers: \n",
    "- **StandardScalerTransformer** *(Series2Series)*.\n",
    "- **DifferenceNormalizer** *(Series2Series)*: subtract the previous value or divide by it.\n",
    "- **TimeToNumGenerator** and **DateSeasonsGenerator** *(Series2Series)* - generator for seasonal features by dates.\n",
    "- **LabelEncodingTransformer** and **OneHotEncodingTransformer** *(Series2Series)* - encoders for categorical features.\n",
    "- **MissingValuesImputer** *(Series2Series)*.\n",
    "- **LagTransformer** *(Series2Features)* - generator for lags. \n",
    "- **LastKnownNormalizer** *(Features2Features)*: normalize all lags by the last known one: divide by it or subtract.\n",
    "\n",
    "!!!The lag transformer must necessarily be present in the sequential transformer, otherwise the features will not be generated.!!!\n",
    "\n",
    "Finally, to generate targets, you need to use **TargetGenerator**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers must be assembled in order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __SeriesToSeries__ transformers should come first, followed by the LagTransformer and TargetGenerator (__SeriesToFeatures__), and then the __FeaturesToFeatures__ transformers.\n",
    "\n",
    "!!!Thus, **StandardScalerNormalizer** and **DifferenceNormalizer** should be before **LagTransformer** and **LastKnownNormalizer** after it!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to build a Pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_easy_params = {\n",
    "    \"target_lags\": 3,\n",
    "    \"date_lags\": 1,\n",
    "#    \"exog_lags\": 1,  # Uncomment this line if you have exogenous features\n",
    "    # One from [\"none\", \"standard_scaler\", \"difference_normalizer\", \"last_known_normalizer\"]\n",
    "    \"target_normalizer\": \"standard_scaler\",\n",
    "    # One from [\"none\", \"delta\", \"ratio\"]  (MUST BE \"none\" for \"standard_scaler\" and NOT \"none\" for others)\n",
    "    \"target_normalizer_regime\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline.easy_setup(dataset_params, pipeline_easy_params, multivariate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can I use exogenous variables in the pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Exogenous variables can also be specified here. Just include them in your pipeline.\n",
    "\n",
    "However, their operation is currently tested only for the `MIMOStrategy` in global-modelling. For other strategies support of additional variables is under development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_params[\"exog_group_1\"] = {\n",
    "#     \"columns\": [\"value\"],\n",
    "#     \"features\": {\n",
    "#         \"StandardScalerTransformer\":\n",
    "#             {\n",
    "#                 \"transform_target\": False, \n",
    "#                 \"transform_features\": True\n",
    "#             },\n",
    "#         \"LagTransformer\": {\"lags\": 7},\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Make sure you have the transform_target = False flag for exogenous features!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model, Validator and Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Model`:\n",
    "  - The model is separate from the strategy. Any model can be run in any strategy if it supports this input and output format.\n",
    "  - You can use on of the implemented ML models (for instance, GBM (Gradient Boosting Machine)).\n",
    "- `Validator`:\n",
    "  - The validator is responsible for setting up the validation process, which includes creating training and validation folds. It ensures that the data is split correctly so that the model’s performance can be accurately assessed. \n",
    "- `Trainer`:\n",
    "  - The trainer is the component that trains the model with provided validator. \n",
    "  - It is necessary to choose a trainer in accordance with the type of model (ML, DL, stats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model parameters\n",
    "model = CatBoost\n",
    "model_params = {\n",
    "    \"loss_function\": \"MultiRMSE\",\n",
    "    \"early_stopping_rounds\": 100,\n",
    "    \"verbose\": 500,\n",
    "}\n",
    "\n",
    "# Configure the validation parameters\n",
    "validation = KFoldCrossValidator\n",
    "validation_params = {\n",
    "    \"n_splits\": 2,\n",
    "}\n",
    "\n",
    "trainer_params = {}\n",
    "\n",
    "trainer = MLTrainer(\n",
    "    model,\n",
    "    model_params,\n",
    "    validation,\n",
    "    validation_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Recursive:_ \n",
    "    - one model for all points of the forecast horizon;\n",
    "    - *training*: the model is trained to predict one point ahead;\n",
    "    - *prediction*: a prediction is iteratively made one point ahead, and then this prediction is used to further shape the features in the test data. \n",
    "- _Recursive-reduced:_\n",
    "    - one model for all points in the prediction horizon;\n",
    "    - *training*: the model is trained to predict one point ahead;\n",
    "    - *prediction*: features are generated for all test observations at once, unavailable values are replaced by NaN.\n",
    "- _Direct:_ \n",
    "    - individual models for each point in the prediction horizon. \n",
    "- _MultiOutput (MIMO - Multi-input-multi-output):_\n",
    "    - one model that learns to predict the entire prediction horizon. \n",
    "    - __Also, this strategy supports the presence of `exogenous features` (only for local- or global-modelling).__\n",
    "- _FlatWideMIMO:_.\n",
    "    - mixture of Direct and MIMO, fit one model, but uses deployed over horizon Direct's features.\n",
    "    - __Number of `lags for datetime features` should be equal to `horizon` while using this strategy.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 3\n",
    "history = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = RecursiveStrategy(horizon, history, trainer, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.indexes.datetimes.DatetimeIndex'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TimedeltaIndex' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fit_time, _ \u001b[38;5;241m=\u001b[39m \u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tsururu_hppl/tsururu/strategies/utils.py:8\u001b[0m, in \u001b[0;36mtiming_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      7\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 8\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m     execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Desktop/tsururu_hppl/tsururu/strategies/recursive.py:71\u001b[0m, in \u001b[0;36mRecursiveStrategy.fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;129m@timing_decorator\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     60\u001b[0m     dataset: TSDataset,\n\u001b[1;32m     61\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursiveStrategy\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fits the recursive strategy to the given dataset.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     features_idx \u001b[38;5;241m=\u001b[39m \u001b[43mindex_slicer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_idx_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_horizon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     target_idx \u001b[38;5;241m=\u001b[39m index_slicer\u001b[38;5;241m.\u001b[39mcreate_idx_target(\n\u001b[1;32m     81\u001b[0m         dataset\u001b[38;5;241m.\u001b[39mdata,\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_horizon,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m         delta\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mdelta,\n\u001b[1;32m     87\u001b[0m     )\n\u001b[1;32m     89\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mcreate_data_dict_for_pipeline(dataset, features_idx, target_idx)\n",
      "File \u001b[0;32m~/Desktop/tsururu_hppl/tsururu/dataset/slice.py:365\u001b[0m, in \u001b[0;36mIndexSlicer.create_idx_data\u001b[0;34m(self, data, horizon, history, step, ids, date_column, delta)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Find indices that, when applied to the original dataset,\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    can be used to obtain windows for building\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    train observations' features.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m \n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 365\u001b[0m     ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mids_from_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m seq_idx_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ids(\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_idx_data,\n\u001b[1;32m    369\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     history \u001b[38;5;241m+\u001b[39m horizon,\n\u001b[1;32m    375\u001b[0m )\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m seq_idx_data\n",
      "File \u001b[0;32m~/Desktop/tsururu_hppl/tsururu/dataset/slice.py:201\u001b[0m, in \u001b[0;36mIndexSlicer.ids_from_date\u001b[0;34m(self, data, date_column, delta, return_delta)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mids_from_date\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m     data: pd\u001b[38;5;241m.\u001b[39mDataFrame,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m     return_delta: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    185\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    186\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find indexes by which the dataset can be divided into\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m        segments that are \"identical\" in terms of time stamps, but\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m        different in terms of some identifier.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     _, time_delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdate_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     ids \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    203\u001b[0m         np\u001b[38;5;241m.\u001b[39margwhere(\n\u001b[1;32m    204\u001b[0m             data[date_column][\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m!=\u001b[39m (data[date_column] \u001b[38;5;241m+\u001b[39m time_delta)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    205\u001b[0m         )\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    207\u001b[0m     )\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_delta:\n",
      "File \u001b[0;32m~/Desktop/tsururu_hppl/tsururu/dataset/slice.py:86\u001b[0m, in \u001b[0;36mIndexSlicer.timedelta\u001b[0;34m(self, x, delta, return_freq_period_info)\u001b[0m\n\u001b[1;32m     84\u001b[0m inferred_freq \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39minfer_freq(x[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:])  \u001b[38;5;66;03m# Need at least 3 dates to infer frequency\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(x))\n\u001b[0;32m---> 86\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# N Years\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m delta \u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m360\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (delta\u001b[38;5;241m.\u001b[39mdays \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m365\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m delta\u001b[38;5;241m.\u001b[39mdays \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m366\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TimedeltaIndex' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "fit_time, _ = strategy.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_time, current_pred = strategy.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>1994.783481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>1994.440142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>1993.71401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>2994.783481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>2994.440142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>2993.71401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>3994.783481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>3994.440142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>3993.71401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>4994.783481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>4994.440142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>4993.71401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>5994.783481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>5994.440142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>5993.71401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>6994.783481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>6994.440142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>6993.71401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>7994.783481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>7994.440142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>7993.71401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>8994.783481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>8994.440142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>8993.71401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>9994.783481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>9994.440142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>9993.71401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>10994.783481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9</td>\n",
       "      <td>2022-09-28</td>\n",
       "      <td>10994.440142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9</td>\n",
       "      <td>2022-09-29</td>\n",
       "      <td>10993.71401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       date         value\n",
       "0   0 2022-09-27   1994.783481\n",
       "1   0 2022-09-28   1994.440142\n",
       "2   0 2022-09-29    1993.71401\n",
       "3   1 2022-09-27   2994.783481\n",
       "4   1 2022-09-28   2994.440142\n",
       "5   1 2022-09-29    2993.71401\n",
       "6   2 2022-09-27   3994.783481\n",
       "7   2 2022-09-28   3994.440142\n",
       "8   2 2022-09-29    3993.71401\n",
       "9   3 2022-09-27   4994.783481\n",
       "10  3 2022-09-28   4994.440142\n",
       "11  3 2022-09-29    4993.71401\n",
       "12  4 2022-09-27   5994.783481\n",
       "13  4 2022-09-28   5994.440142\n",
       "14  4 2022-09-29    5993.71401\n",
       "15  5 2022-09-27   6994.783481\n",
       "16  5 2022-09-28   6994.440142\n",
       "17  5 2022-09-29    6993.71401\n",
       "18  6 2022-09-27   7994.783481\n",
       "19  6 2022-09-28   7994.440142\n",
       "20  6 2022-09-29    7993.71401\n",
       "21  7 2022-09-27   8994.783481\n",
       "22  7 2022-09-28   8994.440142\n",
       "23  7 2022-09-29    8993.71401\n",
       "24  8 2022-09-27   9994.783481\n",
       "25  8 2022-09-28   9994.440142\n",
       "26  8 2022-09-29    9993.71401\n",
       "27  9 2022-09-27  10994.783481\n",
       "28  9 2022-09-28  10994.440142\n",
       "29  9 2022-09-29   10993.71401"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtest validation of pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9617997\ttest: 0.9655311\tbest: 0.9655311 (0)\ttotal: 978us\tremaining: 977ms\n",
      "500:\tlearn: 0.0039542\ttest: 0.0041461\tbest: 0.0041461 (500)\ttotal: 390ms\tremaining: 388ms\n",
      "999:\tlearn: 0.0024434\ttest: 0.0026360\tbest: 0.0026360 (999)\ttotal: 844ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.002636039164\n",
      "bestIteration = 999\n",
      "\n",
      "Fold 0. Score: 0.0026360391639954407\n",
      "0:\tlearn: 0.9647074\ttest: 0.9625819\tbest: 0.9625819 (0)\ttotal: 817us\tremaining: 817ms\n",
      "500:\tlearn: 0.0044702\ttest: 0.0046456\tbest: 0.0046456 (500)\ttotal: 346ms\tremaining: 345ms\n",
      "999:\tlearn: 0.0025553\ttest: 0.0027601\tbest: 0.0027601 (999)\ttotal: 671ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.002760124255\n",
      "bestIteration = 999\n",
      "\n",
      "Fold 1. Score: 0.002760124255276898\n",
      "Mean score: 0.0027\n",
      "Std: 0.0001\n"
     ]
    }
   ],
   "source": [
    "ids, test, pred, fit_time, forecast_time = strategy.back_test(dataset, cv=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_true_1</th>\n",
       "      <th>y_pred_1</th>\n",
       "      <th>id_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997.0</td>\n",
       "      <td>1994.048102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998.0</td>\n",
       "      <td>1994.405436</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999.0</td>\n",
       "      <td>1995.303323</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2997.0</td>\n",
       "      <td>2994.048102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2998.0</td>\n",
       "      <td>2994.405436</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2999.0</td>\n",
       "      <td>2995.303323</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3997.0</td>\n",
       "      <td>3994.048102</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3998.0</td>\n",
       "      <td>3994.405436</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3999.0</td>\n",
       "      <td>3995.303323</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4997.0</td>\n",
       "      <td>4994.048102</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>4994.405436</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4999.0</td>\n",
       "      <td>4995.303323</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5997.0</td>\n",
       "      <td>5994.048102</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5998.0</td>\n",
       "      <td>5994.405436</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5999.0</td>\n",
       "      <td>5995.303323</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6997.0</td>\n",
       "      <td>6994.048102</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6998.0</td>\n",
       "      <td>6994.405436</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6999.0</td>\n",
       "      <td>6995.303323</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7997.0</td>\n",
       "      <td>7994.048102</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7998.0</td>\n",
       "      <td>7994.405436</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7999.0</td>\n",
       "      <td>7995.303323</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8997.0</td>\n",
       "      <td>8994.048102</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8998.0</td>\n",
       "      <td>8994.405436</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8999.0</td>\n",
       "      <td>8995.303323</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9997.0</td>\n",
       "      <td>9994.048102</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9998.0</td>\n",
       "      <td>9994.405436</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9999.0</td>\n",
       "      <td>9995.303323</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10997.0</td>\n",
       "      <td>10994.048102</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10998.0</td>\n",
       "      <td>10994.405436</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10999.0</td>\n",
       "      <td>10995.303323</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y_true_1      y_pred_1 id_1\n",
       "0     1997.0   1994.048102    0\n",
       "1     1998.0   1994.405436    0\n",
       "2     1999.0   1995.303323    0\n",
       "3     2997.0   2994.048102    1\n",
       "4     2998.0   2994.405436    1\n",
       "5     2999.0   2995.303323    1\n",
       "6     3997.0   3994.048102    2\n",
       "7     3998.0   3994.405436    2\n",
       "8     3999.0   3995.303323    2\n",
       "9     4997.0   4994.048102    3\n",
       "10    4998.0   4994.405436    3\n",
       "11    4999.0   4995.303323    3\n",
       "12    5997.0   5994.048102    4\n",
       "13    5998.0   5994.405436    4\n",
       "14    5999.0   5995.303323    4\n",
       "15    6997.0   6994.048102    5\n",
       "16    6998.0   6994.405436    5\n",
       "17    6999.0   6995.303323    5\n",
       "18    7997.0   7994.048102    6\n",
       "19    7998.0   7994.405436    6\n",
       "20    7999.0   7995.303323    6\n",
       "21    8997.0   8994.048102    7\n",
       "22    8998.0   8994.405436    7\n",
       "23    8999.0   8995.303323    7\n",
       "24    9997.0   9994.048102    8\n",
       "25    9998.0   9994.405436    8\n",
       "26    9999.0   9995.303323    8\n",
       "27   10997.0  10994.048102    9\n",
       "28   10998.0  10994.405436    9\n",
       "29   10999.0  10995.303323    9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results(cv=1, regime=\"global\", y_true=test, y_pred=pred, ids=ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with raw time series' granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series come in different granularities, from hourly and daily time series to more complex ones such as the end of each quarter.\n",
    "\n",
    "If the rows do not contain segments that are too short (that are shorter than history + horizon), then `tsururu` will try to extract the row granularity on its own. We currently support the following types:\n",
    "\n",
    "- Yearly (and YearlyEnd)\n",
    "- Quarterly (and Quarterly)\n",
    "- Monthly (and MonthlyEnd)\n",
    "- Weekly\n",
    "- Daily\n",
    "- Hourly\n",
    "- Minlutely\n",
    "- Secondly\n",
    "- Microsecondly\n",
    "\n",
    "There is also support for compound granularities (10 days, 15 minutes, 32 seconds, etc.). The correctness of the selected granularity can be checked from the output after the `Dataset` class has been created.\n",
    "\n",
    "However, there are tricky situations (e.g. 28 days) where the monthly granularity may be guessed incorrectly. Therefore, it is possible to set your own granularity using the `pd.DateOffset` class or related classes from `pandas.tseries.offsets`, which must be fed as `delta` parameter into the `Dataset` class. Then the time column will be processed according to the user's settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a time series where each point is exactly __28 daily points away__ from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path_2 = \"datasets/global/simulated_data_to_check_28D.csv\"\n",
    "\n",
    "# Configure the features settings\n",
    "dataset_params_2 = {\n",
    "    \"target\": {\n",
    "        \"columns\": [\"value\"],\n",
    "        \"type\": \"continious\",\n",
    "    },\n",
    "    \"date\": {\n",
    "        \"columns\": [\"date\"],\n",
    "        \"type\": \"datetime\",\n",
    "    },\n",
    "    \"id\": {\n",
    "        \"columns\": [\"id\"],\n",
    "        \"type\": \"categorical\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq: Month; period: 1.0\n"
     ]
    }
   ],
   "source": [
    "dataset_2 = TSDataset(\n",
    "    data=pd.read_csv(df_path_2),\n",
    "    columns_params=dataset_params_2,\n",
    "    print_freq_period_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the frequency of the series is incorrectly defined as monthly. Let's try to pass the `delta` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom OffSet: <DateOffset: days=28>\n"
     ]
    }
   ],
   "source": [
    "dataset_2 = TSDataset(\n",
    "    data=pd.read_csv(df_path_2),\n",
    "    columns_params=dataset_params_2,\n",
    "    delta=pd.DateOffset(days=28),\n",
    "    print_freq_period_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's all detected correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabpfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
